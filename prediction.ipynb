{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('processed_dataset.csv')\n",
    "# Drop 'Customer Feedback' and 'Recommendation' as they are not used for prediction\n",
    "df = df.drop(columns=['Customer Feedback', 'Recommendation'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Clean numerical columns by replacing non-numeric values with NaN and then filling with mean\n",
    "numerical_cols = ['tenure in months', 'Monthly Average Balance (USD)', 'Yearly Average Balance (USD)']\n",
    "X[numerical_cols] = X[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].mean())\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.75\n",
      "Precision: 0.5588235294117647\n",
      "Recall: 0.35185185185185186\n",
      "F1-Score: 0.4318181818181818\n",
      "ROC-AUC: 0.8205225773718925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Train logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "y_prob_log_reg = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "roc_auc_log_reg = roc_auc_score(y_test, y_prob_log_reg)\n",
    "\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_log_reg}\")\n",
    "print(f\"Precision: {precision_log_reg}\")\n",
    "print(f\"Recall: {recall_log_reg}\")\n",
    "print(f\"F1-Score: {f1_log_reg}\")\n",
    "print(f\"ROC-AUC: {roc_auc_log_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.4444444444444444\n",
      "F1-Score: 0.5333333333333333\n",
      "ROC-AUC: 0.837138508371385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train random forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(f\"Precision: {precision_rf}\")\n",
    "print(f\"Recall: {recall_rf}\")\n",
    "print(f\"F1-Score: {f1_rf}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Feature  Importance\n",
      "16  Monthly Average Balance (USD)    0.176767\n",
      "17   Yearly Average Balance (USD)    0.166616\n",
      "4                tenure in months    0.161167\n",
      "18                       Category    0.100362\n",
      "14             Interest Deposited    0.096229\n",
      "7                    Loan Account    0.032600\n",
      "11            TechSupport Availed    0.028145\n",
      "0                          Gender    0.025429\n",
      "15              Paperless Banking    0.023801\n",
      "6                    Credit Cards    0.021768\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
